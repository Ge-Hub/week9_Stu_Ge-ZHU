Thinking1: 常用的文本分类方法都有哪些?


1.1 传统文本分类方法：
    文本分类问题算是自然语言处理领域中一个经典问题。90年代后互联网在线文本数量增长和机器学习学科的兴起，逐渐形成了一套解决大规模文本分类问题的经典玩法，这个阶段的主要套路是人工特征工程+浅层分类模型。整个文本分类问题就拆分成了特征工程和分类器两部分。机器学习问题是把数据转换成信息再提炼到知识的过程:
    -> 特征是“数据-->信息”的过程，决定了结果的上限。
    文本特征工程分为文本预处理、特征提取、文本表示三个部分，最终目的是把文本转换成计算机可理解的格式，并封装足够用于分类的信息，即很强的特征表达能力。

    -> 分类器是“信息-->知识”的过程，则是去逼近这个上限。
    分类器基本都是统计分类方法，基本上大部分机器学习方法都在文本分类领域有所应用。比如朴素贝叶斯分类算法（Naïve Bayes）、KNN、SVM、最大熵和神经网络等等

1.2 深度学习文本分类方法：

    深度学习最初在之所以图像和语音取得巨大成功，一个很重要的原因是图像和语音原始数据是连续和稠密的，有局部相关性。应用深度学习解决大规模文本分类问题最重要的是解决文本表示，再利用CNN/RNN等网络结构自动获取特征表达能力，去掉繁杂的人工特征工程，端到端的解决问题。

    统做法主要问题的文本表示是高纬度高稀疏的，特征表达能力很弱，而且神经网络很不擅长对此类数据的处理；此外需要人工进行特征工程，成本很高。

    深度学习文本分类模型：
    词向量解决了文本表示的问题，该部分介绍的文本分类模型则是利用CNN/RNN等深度学习网络及其变体解决自动特征提取（即特征表达）的问题。
    -> fastText word2vec: 
        原理是把句子中所有的词向量进行平均（某种意义上可以理解为只有一个avg pooling特殊CNN），然后直接接 softmax 层。其实文章也加入了一些 n-gram 特征的 trick 来捕获局部序列信息。文章倒没太多信息量，算是“水文”吧，带来的思考是文本分类问题是有一些“线性”问题的部分[from项亮]，也就是说不必做过多的非线性转换、特征组合即可捕获很多分类信息，因此有些任务即便简单的模型便可以搞定了。
        fastText 中的网络结果是完全没有考虑词序信息的，而它用的 n-gram 特征 trick 恰恰说明了局部序列信息的重要意义。
        refer to https://arxiv.org/pdf/1607.01759v2.pdf

    -> TextCNN: 
        核心点在于可以捕捉局部相关性，具体到文本分类任务中可以利用CNN来提取句子中类似 n-gram 的关键信息.
        refer to http://colah.github.io/posts/2014-07-Understanding-Convolutions/ 

        TextCNN详细过程：
        第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点了。然后经过有 filter_size=(2,3,4) 的一维卷积层，每个filter_size 有两个输出 channel。第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示了，最后接一层全连接的 softmax 层，输出每个类别的概率。

        特征：这里的特征就是词向量，有静态（static）和非静态（non-static）方式。static方式采用比如word2vec预训练的词向量，训练过程不更新词向量，实质上属于迁移学习了，特别是数据量比较小的情况下，采用静态的词向量往往效果不错。non-static则是在训练过程中更新词向量。推荐的方式是 non-static 中的 fine-tunning方式，它是以预训练（pre-train）的word2vec向量初始化词向量，训练过程中调整词向量，能加速收敛，当然如果有充足的训练数据和资源，直接随机初始化词向量效果也是可以的。

        通道（Channels）：图像中可以利用 (R, G, B) 作为不同channel，而文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。

        一维卷积（conv-1d）：图像是二维数据，经过词向量表达的文本为一维数据，因此在TextCNN卷积用的是一维卷积。一维卷积带来的问题是需要设计通过不同 filter_size 的 filter 获取不同宽度的视野。

        Pooling层：利用CNN解决文本分类问题的文章还是很多的，比如这篇 A Convolutional Neural Network for Modelling Sentences 最有意思的输入是在 pooling 改成 (dynamic) k-max pooling ，pooling阶段保留 k 个最大的信息，保留了全局的序列信息。比如在情感分析场景，举个例子：


    -> TextRNN: 
    
        CNN有个最大问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节也很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 "n-gram" 信息。
    
        RNN算是在自然语言处理领域非常一个标配网络了，在序列标注/命名体识别/seq2seq模型等很多场景都有应用，Recurrent Neural Network for Text Classification with Multi-Task Learning文中介绍了RNN用于分类问题的设计，下图LSTM用于网络结构原理示意图，示例中的是利用最后一个词的结果直接接全连接层softmax输出了。
    
        Refer to https://www.ijcai.org/Proceedings/16/Papers/408.pdf 

    -> TextRNN + Attention: https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf

        CNN和RNN用在文本分类任务中尽管效果显著，但都有一个不足的地方就是不够直观，可解释性不好，特别是在分析badcase时候感受尤其深刻。而注意力（Attention）机制是自然语言处理领域一个常用的建模长时间记忆机制，能够很直观的给出每个词对结果的贡献，基本成了Seq2Seq模型的标配了。实际上文本分类从某种意义上也可以理解为一种特殊的Seq2Seq，所以考虑把Attention机制引入近来.

        Attention的核心point是在翻译每个目标词（或预测商品标题文本所属类别）所用的上下文是不同的，这样的考虑显然是更合理的。
        
        Refer to: https://arxiv.org/pdf/1409.0473v7.pdf  NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE。

    -> TextRCNN（TextRNN + CNN）: 
        利用前向和后向RNN得到每个词的前向和后向上下文的表示
        这样词的表示就变成词向量和前向后向上下文向量concat起来的形式了
        最后再接跟TextCNN相同卷积层，pooling层即可，唯一不同的是卷积层 filter_size = 1就可以了，不再需要更大 filter_size 获得更大视野，这里词的表示也可以只用双向RNN输出

        Refer to: https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552 

    -> Lessens-Learnd

    
　　理论和实践之间的Gap往往差异巨大，学术paper更关注的是模型架构设计的新颖性等，更重要的是新的思路；而实践最重要的是在落地场景的效果，关注的点和方法都不一样。这部分简单梳理实际做项目过程中的一点经验教训。

    模型显然并不是最重要的：不能否认，好的模型设计对拿到好结果的至关重要，也更是学术关注热点。但实际使用中，模型的工作量占的时间其实相对比较少。虽然在第二部分介绍了5种CNN/RNN及其变体的模型，实际中文本分类任务单纯用CNN已经足以取得很不错的结果了，我们的实验测试RCNN对准确率提升大约1%，并不是十分的显著。最佳实践是先用TextCNN模型把整体任务效果调试到最好，再尝试改进模型。
 
    理解你的数据：虽然应用深度学习有一个很大的优势是不再需要繁琐低效的人工特征工程，然而如果你只是把他当做一个黑盒，难免会经常怀疑人生。一定要理解你的数据，记住无论传统方法还是深度学习方法，数据 sense 始终非常重要。要重视 badcase 分析，明白你的数据是否适合，为什么对为什么错。
 
    关注迭代质量 - 记录和分析你的每次实验：迭代速度是决定算法项目成败的关键，学过概率的同学都很容易认同。而算法项目重要的不只是迭代速度，一定要关注迭代质量。如果你没有搭建一个快速实验分析的套路，迭代速度再快也只会替你公司心疼宝贵的计算资源。建议记录每次实验，实验分析至少回答这三个问题：为什么要实验？结论是什么？下一步怎么实验？

    超参调节：超参调节是各位调参工程师的日常了，推荐一篇文本分类实践的论文 A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification，里面贴了一些超参的对比实验，如果你刚开始启动文本分析任务，不妨按文章的结果设置超参，怎么最快的得到超参调节其实是一个非常重要的问题，可以读读 萧瑟的这篇文章 深度学习网络调参技巧 - 知乎专栏。

    一定要用 dropout：有两种情况可以不用：数据量特别小，或者你用了更好的正则方法，比如bn。实际中我们尝试了不同参数的dropout，最好的还是0.5，所以如果你的计算资源很有限，默认0.5是一个很好的选择。
 
    fine-tuning 是必选的：上文聊到了，如果只是使用word2vec训练的词向量作为特征表示，我赌你一定会损失很大的效果。

    未必一定要 softmax loss： 这取决与你的数据，如果你的任务是多个类别间非互斥，可以试试着训练多个二分类器，也就是把问题定义为multi lable 而非 multi class，我们调整后准确率还是增加了>1%。

    类目不均衡问题：基本是一个在很多场景都验证过的结论：如果你的loss被一部分类别dominate，对总体而言大多是负向的。建议可以尝试类似 booststrap 方法调整 loss 中样本权重方式解决。

    避免训练震荡：默认一定要增加随机采样因素尽可能使得数据分布iid，默认shuffle机制能使得训练结果更稳定。如果训练模型仍然很震荡，可以考虑调整学习率或 mini_batch_size。

    没有收敛前不要过早的下结论：玩到最后的才是玩的最好的，特别是一些新的角度的测试，不要轻易否定，至少要等到收敛吧。

参考：https://www.leiphone.com/news/201710/lcuWi98knUcroL6j.html 




Thinking2: RNN为什么会出现梯度消失

2.1 定义（梯度消失，梯度爆炸）

对于神经网络的训练，梯度在训练中起到很关键的作用。 

    梯度消失: 如果在训练过程中发生了梯度消失，这也就意味着我们的权重无法被更新，最终导致训练失败。

    梯度爆炸：梯度爆炸所带来的梯度过大，从而大幅度更新网络参数，造成网络不稳定（可以理解为梯度步伐太大）。在极端情况下，权重的值变得特别大，以至于结果会溢出（NaN值）。

    注意，梯度消失和梯度爆炸只会造成神经网络中较浅的网络的权重无法更新（毕竟神经网络中是反向传播）。

2.2 问题

    梯度消失：会导致我们的神经网络中前面层的网络权重无法得到更新，也就停止了学习。

    梯度爆炸：会使得学习不稳定， 参数变化太大导致无法获取最优参数。

    循环神经网络（RNN）：梯度爆炸会导致网络不稳定，使得网络无法从训练数据中得到很好的学习，最好的结果是网络不能在长输入数据序列上学习。

2.3 原因

tanh' =<1, 对于训练过程大部分情况下tanh的导数是小于1的，因为很少情况下会出现tahn的导数是小于1的 ，因为很少情况下会出现WxXj + WsSj-1 + b1 =0. 

    RNN中梯度消失：如果Ws也是一个大于0小于1的值，则当t很大时，j=k+1/t ∑tanh'Ws会趋近于0 

    RNN中梯度爆炸：如果Ws很大时，，j=k+1/t ∑tanh'Ws会趋近于无穷

2.4 解决方案

LSTM通过门机制来解决了这个问题。遗忘门，输入门，输出门，当前单元状态，当前时刻的隐层输出。 

首先三个门的激活函数是sigmoid， 这也就意味着这三个门的输出要么接近于0 ， 要么接近于1。
    -> 当门为1时， 梯度能够很好的在LSTM中传递，很大程度上减轻了梯度消失发生的概率
    -> 当门为0时，说明上一时刻的信息对当前时刻没有影响， 我们也就没有必要传递梯度回去来更新参数了

通过门机制就能够解决梯度的原因: 单元间的传递为0 或 1

参考：
https://zhuanlan.zhihu.com/p/44163528，
https://zhuanlan.zhihu.com/p/28687529，
https://zhuanlan.zhihu.com/p/28749444